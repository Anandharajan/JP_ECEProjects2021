{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aysha - Birds",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-gx8dm0s-8M"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiCEJeV0D-Ba"
      },
      "source": [
        "import os\r\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeRYHyltElK8"
      },
      "source": [
        "%cd /content/gdrive/My Drive/kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sww_6PadEoiN"
      },
      "source": [
        "!kaggle datasets download -d gpiosenka/100-bird-species"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S4N9idvFOto"
      },
      "source": [
        "!unzip 100-bird-species.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp2dyl1KFPz8"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq2RTKDlS5HI"
      },
      "source": [
        "import shutil\r\n",
        "\r\n",
        "shutil.rmtree('/content/gdrive/My Drive/kaggle/CT_NonCOVID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTOdbvWGFJdd"
      },
      "source": [
        "train_directory='/content/gdrive/My Drive/kaggle/train'\r\n",
        "val_directory='/content/gdrive/My Drive/kaggle/valid'\r\n",
        "test_directory='/content/gdrive/My Drive/kaggle/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRF2K0y3LuZM"
      },
      "source": [
        "#Import packages used here:\r\n",
        "# for initial data exploration:\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython.display import Image, display\r\n",
        "import random\r\n",
        "import math\r\n",
        "\r\n",
        "#For modeling and model viewing. \r\n",
        "import tensorflow as tf\r\n",
        "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.utils import plot_model \r\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import to_categorical #Image generator used for transformation to categorical\r\n",
        "from tensorflow.keras.optimizers import Adam, SGD\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\r\n",
        "from tensorflow.keras import backend, models\r\n",
        "#from sklearn.model_selection import train_test_split  #could have used on the consolidated file.\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "from tensorflow.keras.applications import VGG16, MobileNet\r\n",
        "#from keras.applications.vgg16 import decode_predictions\r\n",
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMB0AQC0LvQx"
      },
      "source": [
        "\r\n",
        "BASE_DIR = '/content/gdrive/My Drive/kaggle/'\r\n",
        "print('BASE_DIR contains ', os.listdir(BASE_DIR))\r\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, '/content/gdrive/My Drive/kaggle/train')\r\n",
        "VALIDATION_DIR = os.path.join(BASE_DIR, '/content/gdrive/My Drive/kaggle/valid')\r\n",
        "TEST_DIR = os.path.join(BASE_DIR, '/content/gdrive/My Drive/kaggle/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pufnh4cFM2Uw"
      },
      "source": [
        "#This will establish the prediction groups for the model.\r\n",
        "CATEGORIES = os.listdir(TRAIN_DIR)\r\n",
        "print(str(len(CATEGORIES)),'CATEGORIES are ', CATEGORIES)\r\n",
        "\r\n",
        "Category_count = len(CATEGORIES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV-Us46wNFoF"
      },
      "source": [
        "#Load an image and determine image shape for analysis.\r\n",
        "IMAGE = load_img(\"/content/gdrive/My Drive/kaggle/train/ANNAS HUMMINGBIRD/025.jpg\")\r\n",
        "plt.imshow(IMAGE)\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "IMAGEDATA = img_to_array(IMAGE)\r\n",
        "SHAPE = IMAGEDATA.shape\r\n",
        "print('Figures are ', SHAPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuhi3jzBNZRB"
      },
      "source": [
        "#This will be used on training, test, and valid data\r\n",
        "General_datagen = ImageDataGenerator(rescale=1./255, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSw6psADNyU0"
      },
      "source": [
        "train_data = General_datagen.flow_from_directory(TRAIN_DIR, target_size=(224,224))\r\n",
        "print('data groups:', len(train_data)) #Will be used to determine steps_per_epoch in my models.\r\n",
        "Train_groups = len(train_data)\r\n",
        "validation_data = General_datagen.flow_from_directory(VALIDATION_DIR, target_size=(224,224),)\r\n",
        "image_qty = len(validation_data.filenames)\r\n",
        "print('data groups:', len(validation_data))\r\n",
        "print('validation image qty:',str(image_qty))\r\n",
        "Valid_groups = len(validation_data)\r\n",
        "test_data = General_datagen.flow_from_directory(TEST_DIR, target_size=(224,224),)\r\n",
        "print('data groups:', len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dIbpTapN3ry"
      },
      "source": [
        "#create seperate labels for images \r\n",
        "def label_images2(DIR, dataset):\r\n",
        "    label = []\r\n",
        "    image = []\r\n",
        "    j=0\r\n",
        "    for i in range (0,30):\r\n",
        "        j = random.randint(0, len(dataset.filenames))\r\n",
        "        label.append(dataset.filenames[j].split('/')[0])\r\n",
        "        image.append(DIR + '/' + dataset.filenames[j])\r\n",
        "    return [label,image]\r\n",
        "\r\n",
        "#plot the random images.\r\n",
        "y,x = label_images2(TEST_DIR, test_data)\r\n",
        "\r\n",
        "for i in range(0,6):\r\n",
        "    X = load_img(x[i])\r\n",
        "    plt.subplot(2,3,+1 + i)\r\n",
        "    plt.axis(False)\r\n",
        "    plt.title(y[i], fontsize=8)\r\n",
        "    plt.imshow(X)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWlTqDMyN-yW"
      },
      "source": [
        "#This was my Sequential model from the CIFAR10 dataset - seemed like a good starting point. -65% accuracy\r\n",
        "#With 2 epochs I got: Test loss: 2.3443613751181243 Test accuracy: 0.4788889\r\n",
        "#With 50 epochs/stopped at 13 Test loss: 1.7568193797407479, Test accuracy: 0.5733333..Not so great. I will move on to pretrained models.\r\n",
        "#Increased from 32 to 64 nodes in CONV2D layers: Test loss: 4.270853807186258, Test accuracy: 0.5377778\r\n",
        "#Changed from Adam to sgd for optimizer:Test loss: 1.4400342908398858, Test accuracy: 0.65444446 - 65%\r\n",
        "backend.clear_session()\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Conv2D(64, (3, 3), padding='same',input_shape=SHAPE)) #224X224\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "\r\n",
        "model.add(Conv2D(64, (3, 3))) #222x222\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #111x111\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.35)) #Doesn't appear to be working in the model summary.\r\n",
        "\r\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(BatchNormalization()) \r\n",
        "\r\n",
        "model.add(Conv2D(64, (3, 3))) #109x109\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #54x54\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.35)) #64 --> 42\r\n",
        "\r\n",
        "model.add(Conv2D(64, (3, 3), padding='same')) #54x54\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "\r\n",
        "model.add(Flatten()) \r\n",
        "model.add(Dropout(0.5)) \r\n",
        "model.add(Dense(512)) \r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dense(Category_count)) #Updated for number of classes\r\n",
        "model.add(Activation('softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "#Compile\r\n",
        "model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\r\n",
        "               loss = 'categorical_crossentropy',\r\n",
        "               metrics = ['accuracy'])\r\n",
        "#fit model\r\n",
        "history = model.fit_generator( \r\n",
        "    train_data, \r\n",
        "    steps_per_epoch = Train_groups, \r\n",
        "    epochs = 50,\r\n",
        "    validation_data = validation_data,\r\n",
        "    validation_steps = Valid_groups,\r\n",
        "    verbose = 1,\r\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights = True),\r\n",
        "               ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, #0.2 to 0.5 dropped to fast 0.7\r\n",
        "                                 patience = 2, verbose = 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBdgvw6OOK0H"
      },
      "source": [
        "#plot accuracy vs epoch\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('Model accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Plot loss values vs epoch\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('Model loss')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Evaluate against test data.\r\n",
        "scores = model.evaluate(test_data, verbose=1)\r\n",
        "print('Test loss:', scores[0])\r\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}